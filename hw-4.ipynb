{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSSM and beyond\n",
    "\n",
    "Повторяем идею из [Learning Deep Structured Semantic Models for Web Search using Clickthrough Data](https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/)\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/v-liaha/v-liaha.github.io/master/assets/dssm.png\" width=600>\n",
    "\n",
    "В качестве энкодера используем **conv - maxpooling**\n",
    "\n",
    "Скачиваем данные [Quora Question Pairs](https://www.kaggle.com/quora/question-pairs-dataset)\n",
    "\n",
    "**Описание данных:**\n",
    "\n",
    "* id - the id of a training set question pair\n",
    "* qid1, qid2 - unique ids of each question (only available in train.csv)\n",
    "* question1, question2 - the full text of each question\n",
    "* is_duplicate - the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alexander/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the GPU to use\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 12\n",
    "vocab_size = 7000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1**\n",
    "\n",
    "Написать функцию, которая приводит строку к нижнему регистру, оставляет запятые, числа, вопросительный и восклицательный знаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`’:]\", \" \", string)  \n",
    "    string = re.sub(r\"’\", \"'\", string) \n",
    "    string = re.sub(r\"`\", \"'\", string) \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\":\", \" : \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "def vectorize(data, tokenizer, time_steps=time_steps):\n",
    "    data = tokenizer.texts_to_sequences(data)\n",
    "    data = pad_sequences(data, maxlen=time_steps, padding='post')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка данных\n",
    "\n",
    "Поменяем постановку задачи: теперь вместо того, чтобы предсказывать, с какой вероятностью данные примеры являются дубликатами, будем находить дубликаты среди пула примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows -- сколько строк с *.csv файла загрузить в память\n",
    "data = pd.read_csv('questions.csv', nrows=50000)\n",
    "\n",
    "# оставляем только дубликаты\n",
    "data = data[data['is_duplicate'] == 1]\n",
    "data = data.dropna()\n",
    "data = data.rename({'question1': 'query', 'question2': 'd+'}, axis=1)\n",
    "\n",
    "# очищаем данные от шума\n",
    "data['query'] = data['query'].apply(lambda x: tokenize_string(x))\n",
    "data['d+'] = data['d+'].apply(lambda x: tokenize_string(x))\n",
    "\n",
    "# создаем K=4 не дубликатов для данного примера\n",
    "data['d1-'] = np.random.permutation(data['d+'].values)\n",
    "data['d2-'] = np.random.permutation(data['d+'].values)\n",
    "data['d3-'] = np.random.permutation(data['d+'].values)\n",
    "data['d4-'] = np.random.permutation(data['d+'].values)\n",
    "\n",
    "# первый пример всегда является дубликатом, все остальные --- нет\n",
    "y = np.zeros((data.shape[0], 5), dtype=int)\n",
    "y[:,0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фитим токенайзер\n",
    "\n",
    "corpus = data['query'].tolist() + data['d+'].tolist()\n",
    "tok = Tokenizer(num_words=vocab_size)\n",
    "tok.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# векторизуем данные\n",
    "\n",
    "q = vectorize(data['query'].values, tok)\n",
    "d0 = vectorize(data['d+'].values, tok)\n",
    "d1 = vectorize(data['d1-'].values, tok)\n",
    "d2 = vectorize(data['d2-'].values, tok)\n",
    "d3 = vectorize(data['d3-'].values, tok)\n",
    "d4 = vectorize(data['d4-'].values, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# делим датасет на обучение и валидацию\n",
    "\n",
    "x = np.hstack((q, d0, d1, d2, d3, d4)).reshape((-1, 6, time_steps))\n",
    "xtr, xev, ytr, yev = train_test_split(x, y, test_size=0.1, random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input_fn\n",
    "\n",
    "С помощью tf.data создаем итератор, который будет подавать данные в модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_x(x):\n",
    "    return {'q': x[:,0],\n",
    "            'd0': x[:,1],\n",
    "            'd1': x[:,2],\n",
    "            'd2': x[:,3],\n",
    "            'd3': x[:,4],\n",
    "            'd4': x[:,5]}\n",
    "\n",
    "# функция, которая подает данные в модель\n",
    "def input_fn(x, labels, params, is_training):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, labels))\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=params['train_size'])\n",
    "        dataset = dataset.repeat(count=params['num_epochs'])\n",
    "\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.map(lambda x, y: (expand_x(x), y))\n",
    "    dataset = dataset.prefetch(buffer_size=100)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2**\n",
    "\n",
    "Реализуйте функцию, котора считает косинусную близость между тензорами размера **(batch_size, dim)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: try to use tf.nn.l2_normalize, tf.multiply\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_sim(x, y):\n",
    "    \"\"\"\n",
    "    Подсчет косинусной близости между двумя тензорами размера (batch_size, dim)\n",
    "    \"\"\"\n",
    "#     cos_sim = tf.losses.cosine_distance(labels=x, predictions=y, axis=1)\n",
    "    dot_product = tf.reduce_sum(tf.multiply(x,y), axis=1)\n",
    "\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3**\n",
    "\n",
    "Реализуйте энкодер, который переводит тензор размера **(batch_size, time_steps, emb_size)** в тензор **(batch_size, new_dim)**\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/73d826d4c2363701b88e3e234fe3b8756c0f9671/3-Figure1-1.png\" width=600>\n",
    "\n",
    "\n",
    "Применить два типа свертки: **[kernel_size=3, strides=2, filters=32], [kernel_size=5, strides=3, filters=32]**\n",
    "\n",
    "Над выходами **average-pooling, max-pooling** соответственно. Полученные тензоры сконкатенировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(features, params, is_training):\n",
    "    \n",
    "    emb_matrix = tf.get_variable('embedding_matrix',\n",
    "                             shape=[params['vocab_size'], params['emb_size']],\n",
    "                             dtype=tf.float32)\n",
    "    \n",
    "    def encode(sentences):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentences: (batch_size, time_steps) последовательности индексов\n",
    "        Returns:\n",
    "            out: (batch_size, new_dim) представление текста в новом пространстве\n",
    "        \"\"\"\n",
    "        \n",
    "        # hints: use tf.nn.embedding_lookup, tf.layers.conv1d, tf.reduce_max\n",
    "        # tf.reduce_mean, tf.concat\n",
    "        embs = tf.nn.embedding_lookup(emb_matrix, sentences)\n",
    "#         sum_vector = tf.reduce_sum(embd, axis=1)\n",
    "#         out = tf.layers.dense(sum_vector, 256, activation=tf.nn.relu)\n",
    "#         out = tf.layers.dense(out, 128, activation=tf.nn.relu)\n",
    "#         out = tf.layers.dense(out, 64)\n",
    "        out = tf.layers.conv1d(inputs=embs, filters=32, kernel_size=3, strides=2)\n",
    "        out = tf.reduce_mean(out, axis=1)\n",
    "        out = tf.layers.conv1d(inputs=embs, filters=32, kernel_size=5, strides=3)\n",
    "        out = tf.reduce_max(out, axis=1)\n",
    "#         out = tf.concat(out, axis=1)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    # энкодим все документы\n",
    "    encoded_features = {}        \n",
    "    \n",
    "    with tf.variable_scope('enc'):\n",
    "        encoded_features['q'] = encode(features['q'])\n",
    "    \n",
    "    for key, value in features.items():\n",
    "        if key != 'q':\n",
    "            with tf.variable_scope('enc', reuse=True):\n",
    "                encoded_features[key] = encode(value)\n",
    "    \n",
    "    # считаем косинусные близости между q и всеми документами\n",
    "    cos_sims = {}\n",
    "    \n",
    "    for key, value in encoded_features.items():\n",
    "        if key != 'q':\n",
    "            cos_sims[key] = cosine_sim(encoded_features['q'], encoded_features[key])\n",
    "    \n",
    "    # конкатинируем косинусные близости\n",
    "    to_concatenate = [cos_sims['d0'], cos_sims['d1'], cos_sims['d2'], cos_sims['d3'], cos_sims['d4']]\n",
    "    concatenated = tf.stack(to_concatenate)\n",
    "    \n",
    "    return concatenated, encoded_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь:\n",
    "\n",
    "$$J(\\theta) = - \\sum_i y_i \\ln(\\hat{y_i})$$\n",
    "\n",
    "Мы хотим, чтобы $cosine\\_similarity(q, d_0) = 1$, а $cosine\\_similarity(q, d_j) = 0$, где $j \\in \\{1,2,3,4\\}$, тогда лосс будет стремиться к нулю.\n",
    "\n",
    "\n",
    "**Задание 4**\n",
    "\n",
    "Реализовать метрики:\n",
    "\n",
    "* Accuracy\n",
    "* MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        logits, _ = build_model(features, params, is_training)\n",
    "        \n",
    "    preds = tf.argmax(logits, axis=1)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {'preds': preds, 'logits': logits}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions)\n",
    "    \n",
    "    # hints: tf.equal, tf.square, tf.substract, tf.cast, tf.reduce_mean\n",
    "    logits = tf.transpose(logits)\n",
    "    labels = tf.to_float(labels)\n",
    "    accuracy, acc_op = tf.metrics.accuracy(labels=tf.argmax(labels, 1), predictions=tf.argmax(logits,1))\n",
    "    mse, mse_op = tf.metrics.mean_squared_error(labels=tf.argmax(labels, 1), predictions=tf.argmax(logits,1))\n",
    "#     print(accuracy)\n",
    "#     print(mse)\n",
    "    \n",
    "#     prediction = tf.argmax(prob, 1)\n",
    "#     equality = tf.equal(prediction, correct_answer)\n",
    "#     accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "    \n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        with tf.variable_scope('metrics'):\n",
    "            eval_metrics = {'accuracy': (accuracy, acc_op),\n",
    "                           'mse': (mse, mse_op)}\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "    \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('mse', mse)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'masha', '_tf_random_seed': 123, '_save_summary_steps': 5, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x117d2c978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'emb_size': 300\n",
    "}\n",
    "\n",
    "config = tf.estimator.RunConfig(tf_random_seed=123,\n",
    "                                model_dir='masha',\n",
    "                                save_summary_steps=5)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn,\n",
    "                                   params=model_params,\n",
    "                                   config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 256,\n",
    "    'num_epochs': 5,\n",
    "    'train_size': int(len(xtr) * 0.9)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into masha/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.6077139, step = 1\n",
      "INFO:tensorflow:global_step/sec: 10.7509\n",
      "INFO:tensorflow:loss = 0.65985274, step = 101 (9.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.306\n",
      "INFO:tensorflow:loss = 0.19193456, step = 201 (9.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0203\n",
      "INFO:tensorflow:loss = 0.09169913, step = 301 (9.980 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 328 into masha/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.06080568.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x117d2c898>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.train(lambda: input_fn(xtr, ytr, params=params, is_training=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-10-21-20:33:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from masha/model.ckpt-328\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-10-21-20:33:16\n",
      "INFO:tensorflow:Saving dict for global step 328: accuracy = 0.7436997, global_step = 328, loss = 0.8626567, mse = 1.8772118\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 328: masha/model.ckpt-328\n",
      "accuracy: 0.7436997294425964\n",
      "loss: 0.8626567125320435\n",
      "mse: 1.8772118091583252\n",
      "global_step: 328\n"
     ]
    }
   ],
   "source": [
    "eval_results = estimator.evaluate(lambda: input_fn(xev, yev, params=params, is_training=False))\n",
    "\n",
    "for key, value in eval_results.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = estimator.predict(lambda: input_fn(xev, yev, params=params, is_training=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from masha/model.ckpt-328\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "logits = []\n",
    "\n",
    "for el in preds:\n",
    "    logits.append(el['logits'])\n",
    "    \n",
    "logits = np.array(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
